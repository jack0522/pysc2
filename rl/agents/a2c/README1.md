# A2CAgent 程式分析

## 檔案結構

- **`agent.py`**: 定義了A2C（Advantage Actor-Critic）演算法的主要類別與方法。
- **`agent_test.py`**: 測試程式，驗證主要演算法的核心功能是否正確實現。

---

## `agent.py`

### **主要類別**
#### `A2CAgent`
該類別實現了一個基於TensorFlow的A2C智能體。其主要功能包括建構神經網絡、執行訓練與推論操作，以及儲存與加載模型。

#### **初始化參數**
- `sess`: TensorFlow會話，用於執行計算圖。
- `network_cls`: 網絡類別（默認為FullyConv）。
- `value_loss_weight`: 值函數損失的權重（默認0.5）。
- `entropy_weight`: 熵損失的權重（默認0.001）。
- `learning_rate`: 學習率（默認7e-4）。
- 其他參數如梯度裁剪範圍、保存檔案數量等。

### **主要方法**

#### `build`
建構神經網絡的計算圖，並初始化相關TensorFlow操作。
- 使用`tf.train.Saver`來保存模型變量。
- 建立訓練摘要以追蹤訓練過程。

#### `_build`
核心計算圖建構函數，定義以下TensorFlow操作：
- **輸入佔位符**: 如`screen`、`minimap`、`flat`等。
- **策略與值函數**: 從`network_cls`中生成的`policy`和`value`。
- **損失函數**:
  - 策略損失（`policy_loss`）：根據優勢值（`advs`）計算。
  - 值函數損失（`value_loss`）：基於返回值（`returns`）。
  - 熵損失（`entropy`）：用於促進探索。
- **優化器**: 使用RMSProp進行優化，並應用梯度裁剪。

#### `train`
執行一次訓練步驟，輸入觀察值、動作、回報值與優勢值，更新模型參數。

#### `step`
根據輸入觀察值執行推論，輸出動作與對應的價值。

#### `save`與`load`
保存與加載模型權重，支援多次訓練後的中斷與恢復。

---

### **輔助函數**

#### `mask_unavailable_actions`
遮罩不可用的動作機率，確保僅能選擇有效動作。

#### `compute_policy_entropy`
計算策略的總熵值，用於衡量模型的隨機性。

#### `sample_actions`
從策略分佈中抽樣動作，用於執行環境交互。

#### `compute_policy_log_probs`
計算動作對應的對數機率，用於計算策略損失。

---

## `agent_test.py`

### **測試內容**

#### 測試 `compute_policy_log_probs`
- 驗證計算對數機率是否符合預期。
- 測試案例模擬了多種情況下的策略分佈與選擇動作。

#### 測試 `compute_policy_entropy`
- 驗證計算策略熵是否正確。
- 通過比較期望值與實際輸出，確保函數的正確性。

### **測試框架**
- 使用`tf.test.TestCase`作為基底類。
- 通過`self.assertAllClose`進行數值驗證。
- 測試範例模擬了A2C智能體的核心功能。

---

## **總結**

### 優點
- 清晰的模組化設計，便於擴展。
- 支援模型保存與恢復，適合長期訓練任務。
- 詳細的測試用例，增強程式的穩健性。

### 建議改進
- 將TensorFlow 1.x版本升級至2.x以提升效能與可維護性。
- 提供更多的訓練數據可視化工具。

---

# 程式檔案差異分析

## 差異概述

| 特性                | `agent.py`                              | `agent_test.py`                         |
|---------------------|-----------------------------------------|-----------------------------------------|
| **用途**            | 定義A2C智能體的核心邏輯與演算法實現。  | 測試智能體邏輯與方法是否正確運作。     |
| **主要內容**        | - 定義`A2CAgent`類別。                 | - 測試函數`compute_policy_log_probs`與`compute_policy_entropy`。 |
|                     | - 涵蓋模型建構、訓練、推論與儲存等功能。| - 使用固定的測試資料進行單元測試。     |
| **輸出**            | 生成TensorFlow計算圖與模型權重。       | 測試結果與驗證輸出值是否正確。         |
| **依賴模組**        | 包含完整的智能體實現與演算法細節。      | 僅依賴部分函數作為測試對象。           |

---

## `agent.py` 的功能重點

### 1. **實現 A2C 算法核心邏輯**
- 包括策略損失、值函數損失與熵正則化等。
- 支援從環境觀察值生成行動。

### 2. **神經網絡建構**
- 使用`network_cls`（如`FullyConv`）構建策略與價值函數。
- 建立計算圖與相關佔位符，用於進行訓練與推論。

### 3. **模型儲存與恢復**
- 支援保存模型狀態（例如訓練步數與權重）。
- 可以從中斷的訓練進度恢復。

### 4. **主要用途**
- 作為一個可供訓練與推論的框架，能在強化學習環境中運行。

---

## `agent_test.py` 的功能重點

### 1. **單元測試框架**
- 使用`TensorFlow`的`tf.test.TestCase`模組撰寫測試案例。
- 驗證`agent.py`中的函數功能正確性。

### 2. **測試函數**
- 測試 **`compute_policy_log_probs`**：
  - 驗證對數機率的計算是否正確。
  - 使用固定的策略分佈與動作選擇進行模擬。
- 測試 **`compute_policy_entropy`**：
  - 驗證策略熵值的計算是否合理。
  - 確保熵的輸出能反映策略的隨機性。

### 3. **模擬測試資料**
- 定義固定的觀察值、策略分佈與動作，用於進行函數測試。
- 模擬不同狀態下的環境輸出，避免依賴真實訓練數據。

### 4. **主要用途**
- 確保`agent.py`的核心函數能按照設計運行。
- 提供函數性測試，以輔助調試和提高可靠性。

---

## 關鍵差異

### 1. **作用範圍**
- `agent.py` 是智能體的完整實現，包括訓練、推論和模型管理。
- `agent_test.py` 僅聚焦於測試`agent.py`中函數的正確性，無完整的環境互動或訓練邏輯。

### 2. **輸入與輸出**
- `agent.py`需要實時環境數據與觀察值作為輸入，輸出策略與價值估計。
- `agent_test.py`使用靜態模擬數據進行測試，輸出驗證結果。

### 3. **依賴性**
- `agent.py`為完整實現，依賴外部環境和神經網絡結構。
- `agent_test.py`僅依賴於部分函數，模擬簡化的測試場景。

---

## 總結

- **`agent.py` 是核心程式碼，負責實現A2C算法的全部邏輯，適用於實際的強化學習環境。**
- **`agent_test.py` 是測試程式，確保`agent.py`的核心函數在簡化條件下運行正確，適用於除錯與驗證。**
